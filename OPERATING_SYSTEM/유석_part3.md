## 📚 16. Thrashing 이란 무엇인가요?
특정 프로그램을 실행하기 위해 CPU가 메모리 접근을 하는데, 페이지 부재가 자주 발생하는 상황을 Thrashing이라고 한다. 컴퓨터에 여러 프로그램이 동시에 메모리에 올라가게 되고 프로그램마다 메모리 할당량이 다르기 때문에 OS가 특정 프로그램한테 메모리를 너무 적게 할당해줄 때 발생한다.

> 16-1. Thrashing 발생 시, 어떻게 완화할 수 있을까요?

working set 알고리즘 방법이 있습니다. working set의 개수만큼 메모리 용량을 받지 못한다면 그 프로세스의 메모리를 빼앗아서 스레싱을 방지하는 알고리즘입니다. 어떤 프로그램이 실행될 때 메모리에 꼭 올라와 있어야 하는 페이지의 집합을 working set라고 하고, working set 알고리즘이 그것을 보장해줍니다.

pff 알고리즘 방법도 있는데 이 방법으 경우는 프로그램 개수를 조절하면서 스레싱을 방지한다는 것은 동일하지만, working set을 추정하기 보단 페이지 부재율을 보면서 조절한다는 차이가 있습니다.

## 📚 17. 가상 메모리란 무엇인가요?
가상 메모리는 컴퓨터 시스템에서 실제 물리적 메모리의 크기에 관계없이, 프로그램이 사용할 수 있는 메모리 공간을 확장하는 기술입니다. 운영체제가 하드디스크의 일부 공간을 가상 메모리로 활용하여, 물리 메모리가 부족할 때에도 프로그램이 계속 실행될 수 있도록 돕습니다.

> 17-1. 가상 메모리가 가능한 이유가 무엇일까요?

가상 메모리가 가능한 이유는 ‘주소 변환’ 때문입니다. 구체적으로 CPU가 프로그래밍에서 사용하는 주소를 실제 메모리 주소로 변환하는 장치가 있어서 가능합니다.

> 17-2. Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.

CPU가 메모리에 접근한 뒤 MMU가 해당 가상 주소를 변환하려다 실패합니다. 그 후 CPU가 바로 OS에게 page fault알림니다.그리고 필요한 데이터를 디스크에서 읽어오고 페이지 테이블을 갱신합니다. 실패했던 명령을 다시 시도합니다.

> 17-3. 페이지 크기에 대한 Trade-Off를 설명해 주세요.

페이지 크기가 작으면 필요한 만큼만 메로리 할당이 가능하기 때문에 메모리 낭비가 줄며, 프로그램이 자주 사용하는 작은 데이터만 올리니까 캐시 효율이 좋아질 수 있습니다. 단점은 페이지 테이블 크기가 커지고 I/O 작업이 빈번해질 수 있습니다.

> 17-4. 페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?

페이지 크기가 커진다고 해서 무조건 페이지 폴트가 더 많이 발생하는 것은 아닙니다. 페이지 폴트는 메모리에 필요한 데이터가 없을 때 발생하는데 페이지가 크면, 한 번의 디스크 접근으로 더 많은 데이터를 메모리에 가져올 수 있습니다. 즉, 한번 페이지를 불러오면 추후 접근할 데이터까지 이미 메모리에 있을 가능성이 커져서 페이지 폴트 발생 빈도가 감소할 수 있습니다.

> 17-5. 세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?

세그먼테이션은 가상 메모리를 관리하는 또 다른 기법으로, 프로그램을 논리적으로 관련 있는 여러 세그먼트로 나누어 메모리를 할당하는 방식입니다. 세그먼트는 크기가 가변적이며, 각각의 세그먼트를 코드, 데이터, 스택, 힙 등과 같은 논리적인 단위로 나눌 수 있습니다. 

세그먼테이션을 사용해도 가상 메모리는 충분히 사용할 수 있습니다. 각 세그먼트에 대해 가상 주소를 물리 주소로 변환하고 만약 그 세그멘트가 물리 메모리에 없다면, 페이지 폴트처럼 세그먼트 폴트를 발생시켜서 OS가 필요한 세그먼트 데이터를 디스크에서 불러오면 됩니다.

## 📚 18. 세그멘테이션과 페이징의 차이점은 무엇인가요?
여러 차이점이 있는데 먼저, 세그멘테이션은 가변 크기 블록을 사용하는 반면, 페이징은 고정 크기 블록을 사용합니다. 그리고 세그멘테이션은 논리적 단위를 기반으로 하고, 페이징은 물리적 크기를 기반으로 합니다. 페이징은 물리 메모리의 남는 프레임에 적절히 배치되기 때문에 외부 단편화가 생기지 않습니다. 반면에 세그먼테이션은 가변 크기를 가지기 때문에 프로세스가 메모리에 적재되고 해제되는 과정이 반복되면, 메모리 공간에 여러 크기의 빈 공간들이 흩어져 생기게 됩니다. 이러한 공간에 세그먼트를 할당할 수 없어 외부 단편화가 발생할 수 있습니다.

> 18-1. 페이지와 프레임의 차이에 대해 설명해 주세요.

페이지(Page)는 가상 메모리 또는 논리적 메모리에서의 고정 크기 블록을 의미합니다. 프로세스의 주소 공간은 이러한 페이지들로 분할됩니다. 페이지는 프로세스가 인식하는 논리적인 단위입니다.
프레임(Frame)은 물리적 메모리(RAM)에서의 고정 크기 블록을 의미합니다. 물리적 메모리는 이러한 프레임들로 분할되어 있습니다. 프레임은 실제 물리적 메모리에 존재하는 공간입니다.

페이지는 가상 메모리에 존재하고, 프레임은 물리적 메모리에 존재합니다. 그리고 페이지는 프로세스 관점에서의 메모리 단위이고, 프레임은 운영체제 및 하드웨어 관점에서의 메모리 단위입니다.

> 18-2. 내부 단편화와, 외부 단편화에 대해 설명해 주세요.

Partition의 크기가 프로세스의 크기보다 커서 메모리가 남지만, 다른 프로세스가 사용할 수 없는 상태를 내부 단편화라고 말합니다. 남아있는 메모리의 크기가 실행하고자 하는 프로세스보다 크지만, 연속적이지 않은 공간에 존재하여 실행하지 못하는 현상을 외부단편화라고 합니다.

> 18-3. 페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.

가상 주소를 페이지 번호와 오프셋으로 분리합니다. 페이지 테이블을 참조하여 페이지 번호에 해당하는 프레임 번호를 찾습니다. 프레임 번호와 오프셋을 결합하여 실제 물리 주소를 계산합니다.

> 18-4. 어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?

운영체제와 메모리 관리 시스템에서는 페이지 테이블 항목(Page Table Entry, PTE)에 보호 비트(protection bit)를 포함시킵니다. 이 보호 비트 중에는 특히 읽기/쓰기 권한을 나타내는 비트가 있습니다. 이때 쓰기 가능 비트가 존재해야 주소공간을 수정할 수 있습니다.

> 18-5. 32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?

32비트 주소 공간은 총 2^32 바이트, 4GB의 주소를 표현할 수 있고, 페이지 크기는 1KB = 2^10 바이트입니다. 정확한 테이블의 최대 크기는 2^32을 2^10으로 나눈 2^22 정도로 약 400만개입니다.

> 18-6. 32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.

32비트 운영체제는 하나의 메모리 주소를 32비트로 표현하므로 주소 공간이 2^32, 즉 4GB로 제한됩니다. 페이징은 가상 메모리를 관리하는 방법일 뿐, 주소 공간 크기를 확장하지는 못하기 때문에, 물리 메모리 사용량 역시 4GB를 초과할 수 없습니다. 결국 32비트 시스템은 구조적으로 최대 4GB까지만 메모리를 직접 관리할 수 있습니다.

> 18-7. C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?

Segmentation Fault는 프로세스가 허용되지 않은 메모리 영역에 접근할 때 발생하는 오류입니다. 이 개념은 운영체제가 프로세스마다 독립된 메모리 공간을 할당하고 보호하는 방식과 밀접한데, 여기에는 세그멘테이션과 페이징이 모두 관여합니다.

운영체제는 세그먼트와 페이지마다 접근 권한(읽기, 쓰기, 실행)을 설정합니다. 프로그램이 이 권한을 위반하거나 존재하지 않는 주소에 접근하면, 하드웨어가 이를 감지해 운영체제에 알리고, 결국 'Segmentation Fault'가 발생합니다.

따라서 Segmentation Fault는 메모리 보호를 위해 세그멘테이션과 페이징 기법을 사용한 결과로 볼 수 있습니다.

## 📚 19. TLB는 무엇인가요?
TLB는 메모리 접근 횟수를 줄이고자 CPU가 접근한 페이지와 프레임 정보를 저장하는 . TLB는 페이지 및 프레임 단위로 저장하고 있으며, 가상 주소를 물리 메모리 주소로 변환할 때 속도를 높여주는 캐시메모리입니다. 

> 19-1. TLB를 쓰면 왜 빨라지나요?

메모리 접근 횟수가 줄어들기 때문입니다. 기존에는 가상 주소를 물리 주소로 변환하기 위해 메모리에 저장된 page table에 접근한 뒤, page table 기반으로 실제 메모리에 접근하여 실제 물리 메모리에 접근합니다. 

하지만 TLB에 접근하여 물리 메모리와 매핑된 정보가 있다면 바로 물리 메모리에 접근할 수 있기 때문에 더 빠릅니다.

> 19-2. MMU가 무엇인가요?

CPU코어 안에 탑재되어 가상 메모리를 실제 물리 메모리로 변환해주는 장치입니다. MMU는 가상 메모리와 실제 메모리 사이 변환을 위해 TLB를 사용합니다.

> 19-3. TLB와 MMU는 어디에 위치해 있나요?

MMU와 TLB는 모두 CPU내부 또는 CPU와 메모리 사이에 위치하는 하드웨어 장치입니다.

> 19-4. 코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?

멀티코어 시스템에서는 각 코어가 자체 TLB를 가지고 있습니다. 이때 하나의 코어가 페이지 테이블을 변경하거나 메모리 매핑을 수정하면, 다른 코어들의 TLB 내용은 더 이상 유효하지 않게 될 수 있습니다. 이를 해결하기 위해 **TLB shootdown**이라는 메커니즘을 사용합니다.

**TLB shootdown**은 한 코어가 페이지 테이블 변경을 감지했을 때, 다른 코어들에게 인터럽트를 보내어 해당 주소에 대한 TLB 엔트리를 무효화(invalidate)하도록 강제하는 방식입니다. 이 과정을 통해 각 코어의 TLB가 일관된 메모리 매핑 정보를 유지할 수 있게 됩니다.

즉, 멀티코어 환경에서는 운영체제가 페이지 테이블 변경 시 **모든 코어에 TLB 무효화 요청(inter-processor interrupt, IPI)** 을 보내어 동기화를 유지합니다.

> 19-5. TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.

Context Switching이 발생하면 CPU가 현재 실행 중인 프로세스의 문맥(context)을 저장하고, 새로운 프로세스의 문맥을 복원하게 됩니다. 이때 중요한 변화 중 하나가 바로 TLB의 무효화입니다.

TLB는 가상 주소를 물리 주소로 변환한 결과를 캐싱하는데, 이 캐시는 프로세스 고유의 가상 주소 공간에 대한 정보입니다. 다른 프로세스로 전환되면, 이전 프로세스의 가상 주소 매핑 정보는 더 이상 유효하지 않기 때문에, **TLB 엔트리를 모두 무효화**하거나, **프로세스에 해당하는 엔트리만 선별적으로 무효화**해야 합니다.

이를 위해 사용되는 최적화 기법이 있는데, Address Space ID는 현대 CPU에서는 프로세스마다 고유한 ASID를 부여해, TLB 엔트리에 ASID를 함께 저장합니다. Context Switch가 일어나더라도 다른 프로세스의 TLB 엔트리는 남겨두고, 현재 활성화된 ASID만 변경합니다. 덕분에 TLB를 전체 비우지 않고 성능을 유지할 수 있습니다.

## 📚 20. 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.
여러가지 방법이 있는데, 커널이 임계구역 진입 전에 인터럽트를 비활성화하여, 현재 실행 중인 프로세스가 중단되지 않도록 하는 인터럽트 비활성화가 있습니다. 단점은 단일 프로세서 시스템에서만 가능한 방법입니다. 

다음으로 Test-and-Set 방법입니다. CPU가 제공하는 원자적 명령어로 임계구역 진입 여부를 나타내는 flag 변수를 검사하고 동시에 설정합니다. 해당 변수가 true면 대기하고 false이면 true로 바꿔 진입을 허용합니다. 이 방법은 다중 프로세서에서도 원자적 실행을 보장합니다.

> 20-1. volatile 키워드는 어떤 의미가 있나요?

volatile은 컴파일러에게 최적화 대상에서 제외하도록 하는 키워드로써 사용된다. 여기서 최적화 대상은 불필요한 연산을 수행하거나 변경되지 않는 값일 때입니다. volatile은 최적화 대상에서 컴파일러가 최적화하는 상황을 방지하기 위해 사용되며, 항상 변경될 수 있는 값임을 명시하여 메모리에서 값을 읽어오도록 합니다.

> 20-2. 싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?

멀티코어 환경에서는 각 CPU가 공유 메모리에 동시 접근할 수 있기 때문에 원자성을 보장하는 명령어가 필요합니다. 아까 말씀드렸던 Test-and-Set과 같은 명령어를 사용해야 합니다. 또한 메모리 일관성을 지키기 위해 각 코어의 L1/L2 캐시 간 데이터 동기화도 필수입니다. 다른 방법으로는 운영체제 커널이 제공하는 뮤텍스, 세마포어와 같은 동기화 추상화 객체를 통해 임계구역 보호가 가능합니다.

## 📚 21. 페이지 교체 알고리즘에 대해 설명해 주세요.
운영체제는 주기억장치보다 더 큰 용량의 프로그램을 실행하기 위해 프로그램의 일부만 주기억장치에 적재하여 사용한다. 이를 가상메모리 기법이라 한다. 이때 페이징 기법으로 메모리를 관리하는 운영체제에서 필요한 페이지가 주기억장치에 적재되지 않았을 시 어떤 페이지 프레임을 선택하여 교체할 것인지 결정하는 방법이 페이지 교체 알고리즘입니다.

> 21-1. LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?

LRU 알고리즘은 가장 오랫동안 사용하지 않은 페이지를 교체하는 방식으로  시간 지역성이라는 특성을 고려한 알고리즘입니다. 시간 지역성은 최근에 참조된 페이지가 가까운 미래에 참조될 가능성이 높은 성질로 페이지 적재의 효율을 높여줍니다.

> 21-2. LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?

먼저 조회할 때는 O(1)의 시간복잡도로 HashMap을 활용하여 성능을 높여줍니다. 이후 캐쉬 데이터가 오래된 데이터를 식별하기 위해 Doubly Linked List를 사용하여 head에 가까운 노드일수록 가장 최근에 참조된 페이지이고, tail에 가까운 노드일수록 가장 오랫동안 참조되지 않는 페이지의 특성을 사용하여 LRU를 구현할 수 있습니다. 

> 21-3. LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.

구현 방법에서 언급한 Doubly Linked List와 HashMap으로 2개의 데이터 구조를 사용하여 구현 복잡도가 높아지고 많은 공간을 차지하는 단점이 있습니다. 또한 데이터 간의 상관관계를 고려하지 않아 참조 지역성을 무시하고 기존 캐시를 삭제하는 경향이 있습니다. 

또한 LRU는 가장 오래 전에 사용된 데이터를 밀어내므로, 순차 접근 방식에서는 최근 사용한 데이터가 계속 밀려나게 되어 캐시 적중률이 떨어질 수 있습니다. 이를 해결하기 위해 LFU(Least Frequently Used)를 사용하여 가장 적게 사용된 데이터를 제거하여 접근 횟수 기반의 데이터 보존을 수행합니다. 더 나아가 상황에 따라 LRU와 LFU를 결합하여 사용횟수와 마지막 사용 시점 각각에 가중치를 계산하여 사용하는 방법도 있습니다.

## 📚 22. File Descriptor와, File System에 에 대해 설명해 주세요.
File Descriptor는 운영체제가 파일, 소켓, 파이프 등 다양한 입출력 리소스를 식별하기 위해 사용하는 정수 값입니다. 프로세스가 파일을 열면, 커널은 파일 테이블에 해당 파일 정보를 저장하고, 그에 대한 인덱스 번호를 프로세스에게 넘겨주는데 인덱스 번호가 바로 File Descriptor입니다.

> 22-1. I-Node가 무엇인가요?

운영체제에서 사용하는 파일 시스템 중 하나로, 파일의 메타데이터를 저장하는 자료구조입니다. 보통 파일 타입, 접근 권한, 파일 크기 등 파일에 관련된 데이터를 저장합니다.

> 22-2. 프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?

파일 오픈의 겨우 내부적으로 OS의 open() 시스템 콜을 실행합니다. OS는 해당 파일에 대해 File Descriptor를 반환합니다. 버퍼 할당 및 읽기는 사용자 공간 버퍼를 내부적으로 할당하고 블록 단위로 여러 바이트를 한 번에 읽습니다. 버퍼에 읽은 데이터를 호출자가 요청한 만큼만 제공하고 버퍼가 고갈되면 다음 블록 단위로 다시 디스크에서 읽습니다. 마지막으로 close() 호출 시 내부 버퍼를 플러시하고, OS의 close()로 시스템 콜로 FD를 반납합니다.

## 📚 23. 동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.
동기는 진행 중인 작업이 완료될 때까지 기다렸다가 결과를 받는 방식이고, 비동기는 작업을 요청하고 즉시 다음 작업을 수행하고 나중에 결과를 받는 방식입니다. 블로킹은 진행 중인 작업이 끝날 때까지 스레드가 대기 상태에 있고, 논블로킹은 작업이 즉시 응답을 반환하지만, 해당 작업이 완료되지 않았을 수 있습니다.

> 23-1. 그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?

동기 + 논블로킹이나 비동기 + 블로킹 조합은 이론적으로는 가능합니다. 예를 들어 동기 논블로킹은 사용자가 직접 반복적으로 작업 완료 여부를 확인하는 방식이고, 비동기 블로킹은 비동기로 시작했지만 결과를 받을 때 Future.get()처럼 blocking이 발생하는 경우입니다. 그러나 이 둘은 일반적으로 비효율적이거나 비동기의 장점을 희석시킬 수 있어서, 실무에서는 주로 동기+블로킹 혹은 비동기+논블로킹 조합이 선호됩니다.

> 23-2. I/O 멀티플렉싱에 대해 설명해 주세요.

I/O 멀티플렉싱은 하나의 스레드가 여러 I/O 자원을 효율적으로 관리할 수 있도록 해주는 기술입니다. select, poll, epoll 같은 시스템 호출을 통해 구현되며, 특히 epoll은 이벤트 기반 방식으로 많은 수의 클라이언트를 처리할 수 있어 고성능 네트워크 서버 개발에 필수적인 기술입니다. 이를 활용하면 블로킹 없이 수천 개의 연결을 효율적으로 관리할 수 있기 때문에, Node.js나 Nginx 같은 고성능 웹 서버들이 이 방식을 채택하고 있습니다.

> 23-3. 논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?

논블로킹 I/O는 시스템 호출이 즉시 반환되기 때문에, 결과를 수신하기 위해선 상태를 반복 확인하거나, I/O 멀티플렉싱을 통해 준비된 시점에만 read()를 수행해야 합니다. 가장 효율적인 방식은 epoll처럼 커널이 준비된 I/O만 알려주는 방식이며, 이벤트 기반 프레임워크에서는 이와 함께 콜백 함수를 사용해 결과를 수신합니다. 이로써 스레드 낭비 없이 대규모 동시성을 구현할 수 있습니다.
