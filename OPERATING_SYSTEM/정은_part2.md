## **8.1 뮤텍스와 세마포어의 차이점은 무엇인가요?**

두 개념 모두 공유 자원에 대한 접근을 제어하는 동기화 도구입니다.
하지만 동작 방식과 사용 목적에서 몇 가지 차이점이 있습니다.

먼저, **뮤텍스(Mutex)**는 이름 그대로 Mutual Exclusion, 즉 상호배제를 위한 도구입니다.

뮤텍스는 한 번에 하나의 스레드만 공유 자원에 접근할 수 있도록 합니다.
특정 스레드가 뮤텍스를 획득(lock)하면, 다른 스레드는 해당 뮤텍스를 사용할 수 없고 대기 상태가 됩니다.
그리고 뮤텍스를 획득한 스레드만 해제(unlock)할 수 있습니다.
즉, 소유권 개념이 존재합니다.

반면에 **세마포어(Semaphore)**는 좀 더 일반적인 동기화 도구입니다.

세마포어는 내부적으로 정수값(카운터)을 가지고 있으며, 이 값이 0보다 크면 접근 가능하고, 0이면 대기하게 됩니다.
카운터 값을 여러 개로 설정할 수 있기 때문에 동시에 여러 스레드가 접근할 수 있도록 제한할 수도 있습니다.
이런 점에서 보면 뮤텍스는 세마포어의 특수한 형태라고도 할 수 있습니다.

또한, 세마포어는 소유권 개념이 없습니다. 즉, 어떤 스레드가 wait()를 호출하고, 다른 스레드가 signal()을 호출하는 방식도 가능합니다.


## 8.2 이진 세마포어와 뮤텍스의 차이

**한 번에 하나의 스레드만 공유 자원에 접근하도록 제어하는 동기화 도구**라는 점에서 유사하다.
이진 세마포어는 **신호 전달을 통해 스레드 간 동기화**를 할 때 사용하고,
뮤텍스는 **공유 자원의 배타적 보호**가 필요한 임계 구역에서 사용됩니다.

데이터베이스 커넥션 풀이나 스레드풀 같은 구조에서
세마포어나 뮤텍스 같은 개념들이 내부적으로 적용되는 경우를 많이 접하게 되는데요.
이런 동기화 도구를 상황에 맞게 선택하는 것이 성능이나 안정성에 큰 영향을 줄 수 있다고 생각합니다.

## 8.3 Lock 대기 프로세스의 lock 획득 방법
(Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?)

락을 얻기 위해 기다리는 동안 컨텍스트 스위칭 없이 루프를 돌며 계속 락을 확인하는 방식을 Spin Lock이라고 합니다.
일반적인 뮤텍스 락은 lock을 못 얻으면 스레드를 sleep 상태로 전환시키는 반면, 스핀 락은 계속 CPU를 점유하면서 대기합니다.

- 장점
    - 락이 **짧은 시간 안에 해제될 것으로 예상될 경우**, 컨텍스트 스위칭 없이 기다릴 수 있어서 오히려 **성능이 더 좋을 수 있습니다.**
    - 특히 **멀티코어 환경에서 락 경쟁이 적고, 락이 금방 해제되는 경우에 적합**합니다.
    - 락을 잡기 위한 과정에서 스레드 상태 전환 비용(context switch)이 없기 때문에, 고성능 시스템에서는 자주 사용됩니다.
- 단점
    - 첫번째로 **CPU 자원 낭비**입니다. 락을 얻을 때까지 연산을 반복하기 때문에 다른 작업들이 CPU를 쓰지 못하게 됩니다.
    - 스레드 우선순위와 상관없는 경쟁 : 모두 동일하게 루프를 돌면서 경쟁
    - 멀티코어 환경에서 캐시 일관성 유지 비용 :  Spin Lock은 계속해서 공유 변수(예: lock 상태)를 읽고 쓰기 때문에 멀티코어 환경에서는 **CPU 간 캐시 일관성을 유지하기 위한 메모리 버스 트래픽**이 증가합니다. 이로 인해 전체 시스템 성능이 저하될 수 있습니다.
- 해결방안
    - **Backoff 전략**
        - 매번 락을 확인하는 게 아니라, **재시도 간의 간격을 점점 늘리면서 락 상태를 확인하는 방식**
    - **Hybrid Lock** 동작 흐름
        1. **락을 시도한다 (try lock)**
        2. 일정 시간 동안 **Spin**한다 (`while` 루프 돌며 lock 상태를 체크)9
        3. 그 시간이 지나도 lock이 안 풀리면, **sleep** 상태로 전환한다 (block, context switch)
        4. **lock을 보유한 스레드가 unlock() 호출**
        5. **block 상태인 스레드 중 하나를 깨워서 다시 lock 시도**
     
## 8.4  뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?

커널이 락을 관리하면 데드락 제어등 안정성과 락 순서 보장등 다중 프로세스 동기화에는 유리하지만,
시스템 콜 오버헤드로 인해 성능 비용이 발생합니다.
이를 보완하기 위한 **Futex 매커니즘**으로 유저 모드에서 먼저 CAS 기반으로 시도한 뒤,  실패 시(충돌 발생 시) 커널 콜을 수행하는 전략이 널리 사용됩니다.
실무에서도 이런 구조는 Java, Linux, JVM 내부 락 구현에서 자주 접할 수 있습니다.


# 9. **Deadlock 에 대해 설명해 주세요.**

## **9.1 Deadlock 에 대해 설명해 주세요.**

Deadlock은 여러 프로세스 자원을 점유한 채 다른 자원을 기다리는 상황에서, 강제로 자원을 회수하지 못하면 서로 대기 상태가 유지되는 상황이다. 

## **9.2 Deadlock이 발생하기 위한 4가지 조건에 대해 설명해 주세요.**

Deadlock이 발생하려면 다음 네 가지 조건이 **모두 충족**되어야 합니다.

1. **상호 배제(Mutual Exclusion)**
    
    → 자원은 하나의 프로세스만 사용할 수 있어야 합니다.
    
2. **점유 대기(Hold and Wait)**
    
    → 프로세스가 자원을 점유한 채로 다른 자원을 기다리는 상태여야 합니다.
    
3. **비선점(No Preemption)**
    
    → 점유한 자원을 강제로 뺏을 수 없어야 합니다.
    비선점은 한 번 어떤 자원을 점유한 프로세스가 **스스로 자원을 반환하기 전까지는**, 운영체제나 다른 프로세스가 그 자원을 **강제로 뺏을 수 없다는 의미**
    
4. **순환 대기(Circular Wait)**
    
    → 프로세스들이 원형으로 자원을 기다리는 형태가 있어야 합니다.
    

## **9.3 그렇다면 3가지만 충족하면 왜 Deadlock은 발생하지 않나요?**

Deadlock은 네 조건이 **동시에** 충족되어야만 발생합니다. 예를 들어 ‘순환 대기’ 조건만 제거하더라도, 사이클이 생기지 않기 때문에 Deadlock은 발생하지 않습니다. 실제로 Deadlock을 예방할 때는 이 중 한 가지 조건이라도 의도적으로 깨트리는 방식으로 해결합니다.

## **9.4 Deadlock은 어떤 방식으로 예방할 수 있을까요?**

- **순환대기 방지 : 자원 할당 순서를 지정**
- **비선점성을 깨뜨리는 방식 :** 타임아웃을 설정(락 기다리는 시간만 제어)하거나, 특정 시간이 지나면 강제 종료(전체 실행 흐름 제어. 예: Thread.interrupt())
- **은행가 알고리즘(Banker’s Algorithm) :** 안전 상태를 유지하면서 자원을 할당
    - 프로세스에게 자원을 할당하기 전에, 지금 자원을 할당해도 시스템이 안전한 상태인지를 계산한 뒤,
    
    안전하다고 판단되면 자원을 할당하는 Deadlock 예방 알고리즘
    

하지만 이런 방법들은 **성능 저하나 구현 복잡도 증가**라는 트레이드오프가 있어서 실무에서는 잘 쓰이진 않습니다.

## **9.5 왜 현대 운영체제는 Deadlock을 처리하지 않나요?**

현대 OS는 대부분 Deadlock을 **감지하거나 회피하지 않고**, 아예 **방치**하는 경우가 많습니다. 이유는 두 가지입니다.

1. **Deadlock 발생 확률이 낮고**
2. **감지·회피 방식이 성능을 크게 저하시킬 수 있기 때문**입니다.

대신에, 일정 시간 동안 응답이 없으면 **Watchdog Timer**나 **타임아웃** 등을 통해 해당 프로세스를 강제 종료시키는 방식으로 대응합니다.

- **Watchdog Timer, 타임아웃(timeout)**

오래 걸리는 작업이나 멈춘 작업을 감지하고 처리하기 위한 장치.
Deadlock을 직접 해결하진 않지만, 무한 대기 상태를 감지하고 시스템을 안정적으로 유지하는 데 사용.

## 9.6 **Wait-Free와 Lock-Free를 비교해 주세요.**

둘 다 동시성 문제를 해결하기 위한 **Non-blocking** 프로그래밍 기법인데요.

- **Lock-Free :**  여러 스레드 중 최소한 하나는 반드시 작업을 마친다는 보장을 합니다
- **Wait-Free :**  모든 스레드가 제한된 시간 안에 작업을 완료한다는, 더 강한 보장입니다.

# 10 **프로그램이 컴파일되어 실행되는 과정을 간략하게 설명해 주세요.**

> 일반적으로 C 같은 컴파일 언어 기준
> 
> 1. 먼저 **전처리기(preprocessor)**가  컴파일 전에 **소스 코드에서 특정 지시문을 처리한다.** 
> 2. 그 후 **컴파일러**가 소스 코드를 어셈블리어로 변환합니다.
> 3. 이 어셈블리 코드를 **어셈블러**가 기계어로 바꾸고,
> 4. 이후 **링커(linker)**가 여러 오브젝트 파일과 라이브러리를 하나의 실행파일로 연결합니다.
> 5. 마지막으로 **로더(loader)**가 실행 파일을 메모리에 적재하고, 프로세스를 실행합니다.

Java 기준

1. 먼저 **소스 코드(.java)** 를 작성합니다.
    
    이 때 전처리 과정은 없으며, `import` 문은 컴파일러가 처리합니다.
    
2. **`javac` 컴파일러**가 `.java` 파일을 **JVM 바이트코드**로 변환해서 `.class` 파일을 생성합니다.
3. 이후 **클래스 로더(Class Loader)** 가 `.class` 파일을 **JVM의 메모리 영역**으로 로드합니다.
4. JVM은 **바이트코드를 인터프리팅(해석)하거나**, **JIT 컴파일러**를 통해 필요한 부분을 **기계어로 변환해 최적화 실행**합니다.
5. **실행 엔진(Execution Engine)** 이 변환된 코드를 기반으로 실제 명령을 실행하며 프로그램이 동작합니다.

---

## 10.2 **링커와 로더의 차이에 대해 설명해 주세요.**

- **링커**는 컴파일된 오브젝트 파일들을 묶어 실행 파일을 만드는 도구이고요
- **로더**는 이 실행 파일을 메모리에 올리고, 실행을 준비하는 운영체제의 구성 요소입니다.
    
    즉, 링커는 실행 파일을 만들고, 로더는 그것을 메모리에 올려서 실행합니다.
    

## 10.3 **컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.**

> 컴파일 언어는 전체 소스를 한 번에 기계어로 변환해서 실행 파일을 만든 다음 실행하고요,
> 
> 
> 인터프리터 언어는 **한 줄씩 읽고 바로 실행**합니다.
> 
> 컴파일 언어는 실행 속도가 빠르고, 인터프리터 언어는 디버깅이 쉽고 유연한 장점이 있습니다.
> 

## 10.4 **JIT(Just-In-Time Compiler)에 대해 설명해 주세요.**

> JIT은 인터프리터 언어에서 실행 중에 자주 실행되는 코드를 실시간으로 기계어로 변환해서 캐싱하는 기술입니다.
> 
> 
> 자바나 파이썬의 일부 구현체에서 사용되며, 초기 실행은 느리지만 반복 실행 시 성능을 크게 개선할 수 있습니다.
> 

## 10.5 **본인이 사용하는 언어는, 어떤 식으로 컴파일 및 실행되는지 설명해 주세요.**

1. 먼저 **소스 코드(.java)** 를 작성합니다.
    
    이 때 전처리 과정은 없으며, `import` 문은 컴파일러가 처리합니다.
    
2. **`javac` 컴파일러**가 `.java` 파일을 **JVM 바이트코드**로 변환해서 `.class` 파일을 생성합니다.
3. 이후 **클래스 로더(Class Loader)** 가 `.class` 파일을 **JVM의 메모리 영역**으로 로드합니다.
4. JVM은 **바이트코드를 인터프리팅(해석)하거나**, **JIT 컴파일러**를 통해 필요한 부분을 **기계어로 변환해 최적화 실행**합니다.
5. **실행 엔진(Execution Engine)** 이 변환된 코드를 기반으로 실제 명령을 실행하며 프로그램이 동작합니다.

## 10.6.**Python 같은 언어는 CPython, Jython, PyPy 등의 다양한 구현체가 있습니다. 각각은 어떤 차이가 있을까요? 또한, 실행되는 과정도 다를까요?**

> 구현체마다 실행 방식이 조금 다릅니다.
> 
> - **CPython**은 가장 기본적인 구현체로, **바이트코드로 컴파일 후 인터프리팅**합니다.
> - **PyPy**는 **JIT 컴파일러**를 포함하고 있어 반복 실행에 빠르고, 성능이 좋습니다.
> - **Jython**은 자바 기반으로, 파이썬 코드를 **바이트코드로 변환하여 JVM에서 실행**합니다.
>     
>     각 구현체는 내부 구조와 성능, 호환성에 차이가 있으며 실행 방식도 약간씩 다릅니다.
>     

## 10.7 **우리는 흔히 fork(), exec() 시스템 콜을 사용하여 프로세스를 적재할 수 있다고 배웠습니다. 로더의 역할은 이 시스템 콜과 상관있는 걸까요? 아니면 다른 방식으로 프로세스를 적재할 수 있는 건가요?**

연관이 있습니다.
일반적으로 셸은 사용자가 명령을 입력하면, 먼저 fork() 시스템 콜을 호출해서 자식 프로세스를 생성합니다.
그리고 자식 프로세스 안에서 exec() 시스템 콜을 호출하면서, 실행할 프로그램의 경로를 전달합니다.

이때 exec() 내부에서 운영체제의 로더(Loader)가 동작합니다.
로더는 기존 자식 프로세스의 주소 공간을 제거하고, 새로운 실행 파일을 해당 메모리 공간에 **적재(load)**한 뒤,
시작 주소(엔트리 포인트)를 설정하여 실행을 시작합니다.

즉, 로더는 exec() 호출 시점에 함께 작동하며, 프로세스를 적재하는 실질적인 역할을 수행하는 커널의 일부입니다.

# 12. **Thread Safe 하다는 것은 어떤 의미인가요?**

**Thread Safe 하다**는 것은 여러 스레드가 동시에 같은 자원(데이터, 메서드 등)에 접근하더라도 프로그램의 실행 결과가 일관되고 오류가 발생하지 않는 상태를 의미합니다.

즉, **공유 자원에 대한 접근을 안전하게 제어하여 경쟁 조건(Race Condition)을 방지**할 수 있는 코드를 "Thread Safe"하다고 합니다.

---

## 12.2 **Thread Safe를 보장하기 위해 어떤 방법을 사용할 수 있나요?**

대표적인 방법은 아래와 같습니다:

| 방법 | 설명 |
| --- | --- |
| **동기화(Synchronization)** | `synchronized` 키워드 또는 `ReentrantLock` 등을 사용하여 **임계 영역(critical section)**을 보호합니다. |
| **volatile 변수 사용** | **CPU 캐시 대신** **메인 메모리에서 읽도록 강제**하여 **가시성(visibility)** 문제를 해결합니다. |
| **Atomic 클래스** | `AtomicInteger`, `AtomicReference` 등은 **CAS(Compare-And-Swap)** 방식으로 락 없이 원자성 보장

* CAS는 "기대값과 같을 때만 값 변경"하는 원자적 연산 |
| **ThreadLocal 사용** | 각 스레드가 자신만의 복사본을 가지도록 하여 공유 자원을 없애는 방식입니다. |
| **불변 객체(Immutable Object)** | 공유 자원을 변경 불가능하게 설계하여 Race Condition 자체를 제거합니다. |

---

## 12.3 **Peterson's Algorithm이 무엇이며, 한계점에 대해 설명해 주세요.**

**Peterson’s Algorithm**은 두 개의 스레드 간의 상호 배제를 구현하기 위한 알고리즘입니다. 공유 변수와 flag를 사용해서 둘 중 한 스레드만 임계 영역에 진입할 수 있도록 합니다.

**동작 방식 요약**:

- 각 스레드는 자신이 들어가길 원한다는 flag를 true로 설정하고, turn을 상대에게 양보합니다.
- 상대방이 임계 영역에 들어가 있지 않거나 자신의 차례일 경우에만 진입합니다.

**한계점**:

- **두 개의 스레드에서만 동작**하며, 다중 스레드 환경에는 적용이 어렵습니다.
- CPU 캐시와 최적화로 인해 **메모리 가시성 문제가 발생할 수 있어**, 현대 아키텍처에서는 직접 사용되지 않습니다.
- 소프트웨어 방식으로 동작하기 때문에 **하드웨어 수준에서 보장되지 않음**.

---

## 12.4 **Race Condition이 무엇인가요?**

Race Condition(경쟁 상태)이란, 두 개 이상의 스레드가 공유 자원에 동시에 접근하여, 실행 순서에 따라 결과가 달라지는 문제를 말합니다.

예를 들어, 두 스레드가 같은 변수 `count++`를 동시에 실행하면 기대와 달리 값이 하나만 증가할 수 있습니다.

**해결 방법**: 임계 영역 보호, 원자성 보장, 락 또는 CAS 등 사용.

---

## 12.5 **Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?**

꼭 락(lock)을 사용할 필요는 없습니다. 락 없이도 Thread Safe를 보장할 수 있는 대표적인 방법은 다음과 같습니다:

- **Atomic 클래스 사용** → 내부적으로 CAS(Compare-And-Swap) 연산을 사용하여 락 없이 원자성 보장
- **불변 객체(Immutable Object)** → 값을 바꾸지 않고 새 객체를 생성해 상태를 전달하는 방식
- **ThreadLocal** → 각 스레드가 자신만의 독립적인 값을 저장할 수 있도록 해주는 클래스로 공유 자체를 피하는 방식
    - 멀티스레드 환경에서 **공유 자원에 대한 동기화 없이도** 각 스레드에 고유한 데이터를 저장하고 읽고 쓸 수 있기 때문입니다.
    - 대표적인 예는:
        - **사용자 인증 정보 :**Spring Security의 **`SecurityContextHolder`** 가 대표적인 ThreadLocal 사용 예
        - **트랜잭션 정보**
        - **로그 추적 ID (traceId)** 등을 저장할 때 사용
- **함수형 프로그래밍 스타일** → 상태 없이 계산만 하는 순수 함수로 동시성 문제 회피


